{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "966ad518-7129-4e4c-927b-12743b19c66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from biobert_embedding.embedding import BiobertEmbedding\n",
    "from utils import graph_tools\n",
    "from multiprocessing import Pool\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74c9d4de-f4e1-42bf-a6a8-31521cee4c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_drug_names = list(graph_tools.get_unique_drugs())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12f9043d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_processes = 10\n",
    "pool = Pool(num_processes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "073a1da6-f4b9-46d5-9f36-8135c77c6252",
   "metadata": {},
   "source": [
    "## Original pip install biobert-embedding does not work (it's too old)\n",
    "\n",
    "I forked the repo. Install using `pip install git+https://github.com/ariellubonja/biobert_embedding`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2a8fab6-27d9-421c-8c03-d2b7333fa481",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing models/pytorch_model.bin\n",
      "Using existing models/config.json\n",
      "Using existing models/vocab.txt\n"
     ]
    }
   ],
   "source": [
    "## Example 1\n",
    "text = \"Breast cancers with HER2 amplification have a higher risk of CNS metastasis and poorer prognosis.\"\\\n",
    "\n",
    "# Class Initialization (You can set default 'model_path=None' as your finetuned BERT model path while Initialization)\n",
    "biobert = BiobertEmbedding()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0e991397-707e-426e-9e5c-9233e9612949",
   "metadata": {},
   "source": [
    "#### Basic example to make sure it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7584fab5-7f55-45d6-96b9-68efebf2263a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Tokens:  ['breast', 'cancers', 'with', 'her2', 'amplification', 'have', 'a', 'higher', 'risk', 'of', 'cns', 'metastasis', 'and', 'poorer', 'prognosis', '.']\n",
      "Shape of Word Embeddings: 16 x 768\n",
      "Shape of Sentence Embedding =  768\n"
     ]
    }
   ],
   "source": [
    "word_embeddings = biobert.word_vector(text)\n",
    "sentence_embedding = biobert.sentence_vector(text)\n",
    "\n",
    "print(\"Text Tokens: \", biobert.tokens)\n",
    "# Text Tokens:  ['breast', 'cancers', 'with', 'her2', 'amplification', 'have', 'a', 'higher', 'risk', 'of', 'cns', 'metastasis', 'and', 'poorer', 'prognosis', '.']\n",
    "\n",
    "print ('Shape of Word Embeddings: %d x %d' % (len(word_embeddings), len(word_embeddings[0])))\n",
    "# Shape of Word Embeddings: 16 x 768\n",
    "\n",
    "print(\"Shape of Sentence Embedding = \",len(sentence_embedding))\n",
    "# Shape of Sentence Embedding =  768\n",
    "\n",
    "## Example 2\n",
    "sentence_vector1 = biobert.sentence_vector('Breast cancers with HER2 amplification have a higher risk of CNS metastasis and poorer prognosis.')\n",
    "sentence_vector2 = biobert.sentence_vector('Breast cancers with HER2 amplification are more aggressive, have a higher risk of CNS metastasis, and poorer prognosis.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d09e2416-c21a-48d5-ac97-2261f15debed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine similarity: 0.9927560091018677\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine as cosine_distance\n",
    "\n",
    "cosine_sim = 1 - cosine_distance(sentence_vector1, sentence_vector2)\n",
    "print('cosine similarity:', cosine_sim)\n",
    "#cosine similarity: 0.992756187915802"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2d71dce-9fe2-4065-8b30-493cb96e923b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method word_vector in module biobert_embedding.embedding:\n",
      "\n",
      "word_vector(text, handle_oov=True, filter_extra_tokens=True) method of biobert_embedding.embedding.BiobertEmbedding instance\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(biobert.word_vector)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "417978d6-9447-431c-8b53-7e2e2ccb9ea6",
   "metadata": {},
   "source": [
    "### Word Embeddings of Drugs.com drug names (without supporting text)\n",
    "\n",
    "Find the drug names I've crawled [here](https://drive.google.com/drive/folders/1EO659a-tyjfXjKzHk-M0WreZBR14MgRM?usp=sharing)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ad1705e2",
   "metadata": {},
   "source": [
    "<font color=\"orange\">The cell below will try to handle Out of Vocabulary terms. This means that the embedding matrix is not a clear matrix, but will be a list of Tensors</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "509bd66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will take a while\n",
    "\n",
    "# biobert_embeddings_oov = list(map(biobert.word_vector, unique_drug_names)) # Single-threaded\n",
    "\n",
    "# Out of vocabulary will be split into smaller. May end up with multiple embeddings for each word\n",
    "# parallelized\n",
    "biobert_embeddings_oov = pool.map(biobert.word_vector, unique_drug_names)\n",
    "\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc53676f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(biobert_embeddings_oov[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c88e1c0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3310"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# These are the indices of the drug names that have only one embedding, i.e. the drug name is in the Vocabulary\n",
    "vector_embd_indices = [i for i, num in enumerate(biobert_embeddings_oov) if len(num) == 1]\n",
    "len(vector_embd_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc57811b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(biobert_embeddings_oov) == len(unique_drug_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1e28ea46",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_embeddings = torch.stack([biobert_embeddings_oov[i][0] for i in vector_embd_indices]).numpy()\n",
    "corresponding_drug_names = [unique_drug_names[i] for i in vector_embd_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "32cad206",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3310, 768)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_embeddings.numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0fc12e82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>758</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>trilafon</th>\n",
       "      <td>-2.494238</td>\n",
       "      <td>-5.982688</td>\n",
       "      <td>3.391733</td>\n",
       "      <td>-3.165889</td>\n",
       "      <td>-3.972550</td>\n",
       "      <td>-3.434661</td>\n",
       "      <td>2.095402</td>\n",
       "      <td>1.464997</td>\n",
       "      <td>6.532821</td>\n",
       "      <td>-1.086071</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.374193</td>\n",
       "      <td>3.749851</td>\n",
       "      <td>1.355658</td>\n",
       "      <td>-2.396497</td>\n",
       "      <td>3.676836</td>\n",
       "      <td>1.897683</td>\n",
       "      <td>0.468625</td>\n",
       "      <td>6.310996</td>\n",
       "      <td>-9.883236</td>\n",
       "      <td>-1.279147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>calcipotriene</th>\n",
       "      <td>4.240830</td>\n",
       "      <td>1.769997</td>\n",
       "      <td>-0.795535</td>\n",
       "      <td>-6.929158</td>\n",
       "      <td>5.979644</td>\n",
       "      <td>-0.761292</td>\n",
       "      <td>-1.656379</td>\n",
       "      <td>0.054974</td>\n",
       "      <td>0.432746</td>\n",
       "      <td>12.672398</td>\n",
       "      <td>...</td>\n",
       "      <td>-10.860700</td>\n",
       "      <td>-1.191983</td>\n",
       "      <td>-9.798771</td>\n",
       "      <td>-4.564957</td>\n",
       "      <td>-9.084117</td>\n",
       "      <td>-5.654516</td>\n",
       "      <td>-0.909690</td>\n",
       "      <td>10.118820</td>\n",
       "      <td>-6.920544</td>\n",
       "      <td>3.411890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fosinopril</th>\n",
       "      <td>2.749605</td>\n",
       "      <td>-11.437942</td>\n",
       "      <td>2.700483</td>\n",
       "      <td>-0.598100</td>\n",
       "      <td>9.610103</td>\n",
       "      <td>-13.214931</td>\n",
       "      <td>4.708538</td>\n",
       "      <td>7.986394</td>\n",
       "      <td>-0.904664</td>\n",
       "      <td>-3.148977</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.771662</td>\n",
       "      <td>3.917134</td>\n",
       "      <td>-10.183059</td>\n",
       "      <td>5.208779</td>\n",
       "      <td>6.980632</td>\n",
       "      <td>0.385215</td>\n",
       "      <td>2.746163</td>\n",
       "      <td>-2.396154</td>\n",
       "      <td>-15.207134</td>\n",
       "      <td>-8.165861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nadostine</th>\n",
       "      <td>-2.403095</td>\n",
       "      <td>-5.212749</td>\n",
       "      <td>-2.980472</td>\n",
       "      <td>-4.289715</td>\n",
       "      <td>1.065220</td>\n",
       "      <td>-2.463279</td>\n",
       "      <td>2.731372</td>\n",
       "      <td>3.838523</td>\n",
       "      <td>-2.878551</td>\n",
       "      <td>3.916560</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.626741</td>\n",
       "      <td>4.469468</td>\n",
       "      <td>0.329026</td>\n",
       "      <td>1.498180</td>\n",
       "      <td>-2.434177</td>\n",
       "      <td>-1.255941</td>\n",
       "      <td>0.073906</td>\n",
       "      <td>2.629568</td>\n",
       "      <td>-7.653620</td>\n",
       "      <td>3.064509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>duavee</th>\n",
       "      <td>-2.835025</td>\n",
       "      <td>-6.494714</td>\n",
       "      <td>2.831769</td>\n",
       "      <td>-5.364807</td>\n",
       "      <td>4.386308</td>\n",
       "      <td>-0.174934</td>\n",
       "      <td>3.309786</td>\n",
       "      <td>-0.848888</td>\n",
       "      <td>2.427666</td>\n",
       "      <td>7.614682</td>\n",
       "      <td>...</td>\n",
       "      <td>0.924215</td>\n",
       "      <td>1.525765</td>\n",
       "      <td>-0.303652</td>\n",
       "      <td>3.410230</td>\n",
       "      <td>0.926888</td>\n",
       "      <td>-4.464722</td>\n",
       "      <td>2.695533</td>\n",
       "      <td>6.594471</td>\n",
       "      <td>-3.707011</td>\n",
       "      <td>-5.687425</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 768 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    0          1         2         3         4          5    \\\n",
       "trilafon      -2.494238  -5.982688  3.391733 -3.165889 -3.972550  -3.434661   \n",
       "calcipotriene  4.240830   1.769997 -0.795535 -6.929158  5.979644  -0.761292   \n",
       "fosinopril     2.749605 -11.437942  2.700483 -0.598100  9.610103 -13.214931   \n",
       "nadostine     -2.403095  -5.212749 -2.980472 -4.289715  1.065220  -2.463279   \n",
       "duavee        -2.835025  -6.494714  2.831769 -5.364807  4.386308  -0.174934   \n",
       "\n",
       "                    6         7         8          9    ...        758  \\\n",
       "trilafon       2.095402  1.464997  6.532821  -1.086071  ...  -9.374193   \n",
       "calcipotriene -1.656379  0.054974  0.432746  12.672398  ... -10.860700   \n",
       "fosinopril     4.708538  7.986394 -0.904664  -3.148977  ...  -8.771662   \n",
       "nadostine      2.731372  3.838523 -2.878551   3.916560  ...  -5.626741   \n",
       "duavee         3.309786 -0.848888  2.427666   7.614682  ...   0.924215   \n",
       "\n",
       "                    759        760       761       762       763       764  \\\n",
       "trilafon       3.749851   1.355658 -2.396497  3.676836  1.897683  0.468625   \n",
       "calcipotriene -1.191983  -9.798771 -4.564957 -9.084117 -5.654516 -0.909690   \n",
       "fosinopril     3.917134 -10.183059  5.208779  6.980632  0.385215  2.746163   \n",
       "nadostine      4.469468   0.329026  1.498180 -2.434177 -1.255941  0.073906   \n",
       "duavee         1.525765  -0.303652  3.410230  0.926888 -4.464722  2.695533   \n",
       "\n",
       "                     765        766       767  \n",
       "trilafon        6.310996  -9.883236 -1.279147  \n",
       "calcipotriene  10.118820  -6.920544  3.411890  \n",
       "fosinopril     -2.396154 -15.207134 -8.165861  \n",
       "nadostine       2.629568  -7.653620  3.064509  \n",
       "duavee          6.594471  -3.707011 -5.687425  \n",
       "\n",
       "[5 rows x 768 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(vector_embeddings, index=corresponding_drug_names)\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6a40d13a",
   "metadata": {},
   "source": [
    "## Evaluations\n",
    "\n",
    "Top-K similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90695fab",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'faiss'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mfaiss\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39m# Instantiate an index with the desired index type and dimensionality\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'faiss'"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Instantiate an index with the desired index type and dimensionality\n",
    "index = faiss.IndexFlatIP(256)  # Cosine similarity, assuming 256-dimensional embeddings\n",
    "\n",
    "biobert_embeddings_np = vector_embeddings\n",
    "\n",
    "# Add biobert_embeddings_np to the index\n",
    "index.add(biobert_embeddings_np)\n",
    "\n",
    "# Specify the number of nearest neighbors to retrieve (k)\n",
    "k = 5\n",
    "\n",
    "# Perform the similarity search for each embedding\n",
    "D, I = index.search(biobert_embeddings_np, k+1)  # Retrieve k+1 neighbors to exclude self\n",
    "\n",
    "# D contains the similarities to the nearest neighbors (cosine similarity)\n",
    "# I contains the indices of the nearest neighbors (excluding self)\n",
    "\n",
    "# Iterate over each embedding\n",
    "for i in range(biobert_embeddings_np.shape[0]):\n",
    "    embedding = biobert_embeddings_np[i]\n",
    "    nearest_indices = I[i][1:]  # Exclude self, start from index 1\n",
    "    nearest_similarities = D[i][1:]  # Exclude self, start from index 1\n",
    "\n",
    "    # Print the top-k nearest neighbors for the current embedding\n",
    "    print(f\"Embedding {i+1}:\")\n",
    "    for j, index in enumerate(nearest_indices):\n",
    "        similarity = nearest_similarities[j]\n",
    "        print(f\"Nearest Neighbor {j+1}: Index {index}, Similarity {similarity}\")\n",
    "    print()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ef7e3795",
   "metadata": {},
   "source": [
    "## 3194 of the Drugs have vector embeddings, whereas the rest return a Matrix that we have to deal with using Pooling\n",
    "\n",
    "Let's see the 3194"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e16c7ec8",
   "metadata": {},
   "source": [
    "<font color=\"red\">Most of the drugs give a matrix as their embedding, and not a vector.</font>\n",
    "\n",
    "Need to try various pooling methods to get a vector from the matrix + see what is best"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4cb57841-87c8-441c-8847-96abec59cced",
   "metadata": {},
   "source": [
    "## TODO try ClinicalBERT https://github.com/EmilyAlsentzer/clinicalBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdfdda59-4fe6-49f4-ad18-b2c8e248ffef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ariellubonja/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "model = AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83af139c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "encoded_inputs = tokenizer(unique_drug_names, padding=True, truncation=True, return_tensors='pt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "85a3181b",
   "metadata": {},
   "source": [
    "Macbook M1 Pro Runtimes\n",
    "\n",
    "CPU - 1min 17 sec\n",
    "\n",
    "MPS (torch 1.13) - Uses way too much memory (50GB+ - crashes) whereas CPU is max 13GB\n",
    "\n",
    "MPS (torch 2.01) - 1min 15s - 18GB memory usage\n",
    "\n",
    "\n",
    "<font color=\"red\">Conclusion - not useful yet!</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44d8eaa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1min 15s ± 8.66 s per loop (mean ± std. dev. of 3 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -r 3 -n 1\n",
    "\n",
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_inputs)\n",
    "    embeddings = model_output.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fb4c74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 768])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7befa08-cb7f-4b6a-8a4d-dc54462213f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
